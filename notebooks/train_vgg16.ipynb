{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYuwYj1vBa0B",
        "outputId": "bb182f28-c800-4d99-c977-da92ab63d1de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['background', 'thumbs_up', 'v_sign']\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf, pathlib, zipfile, json, os\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "ZIP_PATH      = \"gesture_data.zip\"   # <-- upload this file\n",
        "DATA     = pathlib.Path(\"gesture_data\")\n",
        "IMG_SIZE      = (224, 224)\n",
        "BATCH_SIZE    = 32\n",
        "EPOCHS_HEAD   = 8                         # warm-up head\n",
        "EPOCHS_FINE   = 6                         # fine-tune last block\n",
        "PATIENCE      = 3                         # for early stopping\n",
        "\n",
        "# Unzip dataset\n",
        "zipfile.ZipFile(ZIP_PATH).extractall(DATA)\n",
        "\n",
        "# Count classes automatically\n",
        "DATA_ROOT     = pathlib.Path(\"gesture_data/gesture_data\")\n",
        "class_names = sorted([p.name for p in DATA_ROOT.iterdir() if p.is_dir()])\n",
        "num_classes = len(class_names)\n",
        "assert num_classes in (2,3), f\"Expected 2 or 3 folders, found {num_classes}: {class_names}\"\n",
        "print(\"Classes:\", class_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rescale=1/255.,\n",
        "    validation_split=0.2,\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.10,\n",
        "    height_shift_range=0.10,\n",
        "    brightness_range=[0.7, 1.3],\n",
        "    zoom_range=0.10\n",
        ")\n",
        "\n",
        "train_gen = datagen.flow_from_directory(\n",
        "    DATA_ROOT,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='sparse',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_gen = datagen.flow_from_directory(\n",
        "    DATA_ROOT,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='sparse',\n",
        "    subset='validation'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5FkpYIHBdf-",
        "outputId": "e1c092e2-a352-4ea2-e5d9-f8b9d59aee47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 360 images belonging to 3 classes.\n",
            "Found 90 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base = tf.keras.applications.VGG16(\n",
        "    include_top=False, weights=\"imagenet\",\n",
        "    input_shape=IMG_SIZE + (3,))\n",
        "base.trainable = False\n",
        "\n",
        "model = models.Sequential([\n",
        "    base,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dropout(0.30),\n",
        "    layers.Dense(num_classes, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "es = EarlyStopping(patience=PATIENCE, monitor=\"val_accuracy\",\n",
        "                   restore_best_weights=True, verbose=1)\n",
        "ck = ModelCheckpoint(\"best_head.keras\", monitor=\"val_accuracy\",\n",
        "                     save_best_only=True, verbose=1)\n",
        "\n",
        "history_head = model.fit(train_gen, validation_data=val_gen,\n",
        "                         epochs=EPOCHS_HEAD, callbacks=[es, ck])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKR66wT7BflH",
        "outputId": "a185e6cf-0790-4145-dad0-ed83140c7404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - accuracy: 0.3568 - loss: 1.1435 "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_accuracy improved from -inf to 0.60000, saving model to best_head.keras\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 16s/step - accuracy: 0.3574 - loss: 1.1426 - val_accuracy: 0.6000 - val_loss: 1.0158\n",
            "Epoch 2/8\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - accuracy: 0.5279 - loss: 1.0042 \n",
            "Epoch 2: val_accuracy improved from 0.60000 to 0.66667, saving model to best_head.keras\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 16s/step - accuracy: 0.5294 - loss: 1.0036 - val_accuracy: 0.6667 - val_loss: 0.9385\n",
            "Epoch 3/8\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - accuracy: 0.5999 - loss: 0.9807 \n",
            "Epoch 3: val_accuracy did not improve from 0.66667\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 16s/step - accuracy: 0.5999 - loss: 0.9794 - val_accuracy: 0.6556 - val_loss: 0.8647\n",
            "Epoch 4/8\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - accuracy: 0.6645 - loss: 0.8887 \n",
            "Epoch 4: val_accuracy improved from 0.66667 to 0.67778, saving model to best_head.keras\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 15s/step - accuracy: 0.6640 - loss: 0.8878 - val_accuracy: 0.6778 - val_loss: 0.8079\n",
            "Epoch 5/8\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - accuracy: 0.6707 - loss: 0.8438 \n",
            "Epoch 5: val_accuracy improved from 0.67778 to 0.81111, saving model to best_head.keras\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 16s/step - accuracy: 0.6723 - loss: 0.8420 - val_accuracy: 0.8111 - val_loss: 0.7606\n",
            "Epoch 6/8\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - accuracy: 0.7156 - loss: 0.8024 \n",
            "Epoch 6: val_accuracy improved from 0.81111 to 0.83333, saving model to best_head.keras\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 15s/step - accuracy: 0.7169 - loss: 0.8014 - val_accuracy: 0.8333 - val_loss: 0.7221\n",
            "Epoch 7/8\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - accuracy: 0.7358 - loss: 0.7432 \n",
            "Epoch 7: val_accuracy did not improve from 0.83333\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 15s/step - accuracy: 0.7354 - loss: 0.7423 - val_accuracy: 0.8000 - val_loss: 0.6936\n",
            "Epoch 8/8\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - accuracy: 0.7689 - loss: 0.6971 \n",
            "Epoch 8: val_accuracy did not improve from 0.83333\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 16s/step - accuracy: 0.7672 - loss: 0.6979 - val_accuracy: 0.8222 - val_loss: 0.6599\n",
            "Restoring model weights from the end of the best epoch: 6.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base.trainable = True\n",
        "for layer in base.layers[:-4]:  # freeze all but block5_conv*\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "ck_ft = ModelCheckpoint(\"best_finetune.keras\", monitor=\"val_accuracy\",\n",
        "                        save_best_only=True, verbose=1)\n",
        "es_ft = EarlyStopping(patience=PATIENCE, monitor=\"val_accuracy\",\n",
        "                      restore_best_weights=True, verbose=1)\n",
        "\n",
        "history_ft = model.fit(train_gen, validation_data=val_gen,\n",
        "                       epochs=EPOCHS_FINE, callbacks=[es_ft, ck_ft])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmoBeDTkBhnU",
        "outputId": "e9387762-4abf-4628-87a0-6dbe9407f515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15s/step - accuracy: 0.7243 - loss: 0.7211 \n",
            "Epoch 1: val_accuracy improved from -inf to 0.87778, saving model to best_finetune.keras\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 18s/step - accuracy: 0.7267 - loss: 0.7178 - val_accuracy: 0.8778 - val_loss: 0.5539\n",
            "Epoch 2/6\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15s/step - accuracy: 0.8014 - loss: 0.5557 \n",
            "Epoch 2: val_accuracy improved from 0.87778 to 0.90000, saving model to best_finetune.keras\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 18s/step - accuracy: 0.8023 - loss: 0.5535 - val_accuracy: 0.9000 - val_loss: 0.4533\n",
            "Epoch 3/6\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15s/step - accuracy: 0.8647 - loss: 0.4422 \n",
            "Epoch 3: val_accuracy improved from 0.90000 to 0.93333, saving model to best_finetune.keras\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 18s/step - accuracy: 0.8638 - loss: 0.4414 - val_accuracy: 0.9333 - val_loss: 0.3728\n",
            "Epoch 4/6\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15s/step - accuracy: 0.8798 - loss: 0.3863 \n",
            "Epoch 4: val_accuracy improved from 0.93333 to 0.96667, saving model to best_finetune.keras\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 18s/step - accuracy: 0.8787 - loss: 0.3857 - val_accuracy: 0.9667 - val_loss: 0.3161\n",
            "Epoch 5/6\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17s/step - accuracy: 0.9060 - loss: 0.3052 \n",
            "Epoch 5: val_accuracy did not improve from 0.96667\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 21s/step - accuracy: 0.9071 - loss: 0.3049 - val_accuracy: 0.9667 - val_loss: 0.2473\n",
            "Epoch 6/6\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15s/step - accuracy: 0.9324 - loss: 0.2733 \n",
            "Epoch 6: val_accuracy did not improve from 0.96667\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 18s/step - accuracy: 0.9314 - loss: 0.2733 - val_accuracy: 0.9556 - val_loss: 0.2197\n",
            "Restoring model weights from the end of the best epoch: 4.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "open(\"gestures.tflite\", \"wb\").write(tflite_model)\n",
        "\n",
        "print(\"\\n✅  Training complete.  Validation accuracy:\",\n",
        "      history_ft.history.get(\"val_accuracy\", history_head.history[\"val_accuracy\"])[-1])\n",
        "print(\"📦  Saved gestures.tflite  (download from the left-side file browser)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTqjaGMBBkMA",
        "outputId": "314bbaf3-12b3-456d-d637-d5c3f543ae9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmp1nifcntu'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor_43')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  132638799622800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799621648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799622032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799620880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799621456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799620112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799620688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799619344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799619920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799618576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799619152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799617808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799618384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799617040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799617616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799616272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799616848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799615504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799616080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799614736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799615312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799613968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799614544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799625488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799614160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638799625872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638797840656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132638797842384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "\n",
            "✅  Training complete.  Validation accuracy: 0.9555555582046509\n",
            "📦  Saved gestures.tflite  (download from the left-side file browser)\n"
          ]
        }
      ]
    }
  ]
}